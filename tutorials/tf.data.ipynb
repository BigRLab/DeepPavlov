{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nab/work/DeepPavlov/venv-dp/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datapath = 'train_poor.csv'  # пока что подразумеваем, что загрузка уже произведена"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее мы использовали стандартные input_fn из tf.estimator, чтобы кормить данные MyEstimator, однако далеко не всегда удобно грузить весь датасет в DataFrame pandas или в массивы numpy, которые полностью живут в оперативной памяти, поэтому в tf возможно вместо high-level Estimator API использовать mid-level Dataset API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SkipDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_ds = tf.data.TextLineDataset(train_datapath).skip(1)  # сразу же пропускаем первую строчку с заголовками\n",
    "snips_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно преобразовать каждую строку (каждый элемент выборки) нашего датасета, чтобы уточнить, где в этой строке можно найти фичи, а где разметку для обучения, и как всё это преобразовывать перед подачей на вход модели. Для выполнения такого преобразования нашего snips_ds, как и у любого tf.data.Dataset, есть метод map, принимающий на вход функцию, которую опишем ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_line(line: tf.Tensor): # на входе тензор типа tf.string и это предвещает большие проблемы...\n",
    "    \n",
    "    text_string, label_string = tf.decode_csv(records=line, record_defaults=[[\"\"], [\"\"]])  # есть даже спец. функция\n",
    "    # есть ещё варианты парсить tf.string в tf.train.Example, если кто-то им пользовался\n",
    "    label_string = tf.string_to_hash_bucket_fast(input=label_string, num_buckets=7)  # на этот раз без словаря для лейблов\n",
    "    features = {'text': text_string}  # MyEstimator с помощью ключей словаря матчит данные с feature_column\n",
    "    labels = label_string  # разметку тоже можно паковать в словарь, если MyEstimator поддерживает это\n",
    "    \n",
    "    return features, labels # train_input_fn должна возвращать tf.data.Dataset, состоящий из таких вот кортежей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В принципе можно с помощью tf.py_func() как-то возможно более просто делать преобразования, однако не все такие преобразования будут выполняться на плюсовом бекэнде tf, из-за чего может возникнуть ботлнек, заставляющий GPU простаивать и как следствие замедляющий время обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ({text: ()}, ()), types: ({text: tf.string}, tf.int64)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_ds = snips_ds.map(_parse_line) # map применяет к каждому элементу переданную функцию и возвращает новый объект\n",
    "snips_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы объявили, как будут считаны и обработаны данные, но само считывание будет происходить в момент обучения, чтобы поменять требования по оперативной памяти (датасет туда может не влезать) на требования к вычислительным ресурсам (считывание данных делает процессор, и если операции матричного перемножения выполняются на GPU/TPU, то процессор всё равно не так уж и загружен). Объявим ещё и то, как данные будут подготовлены для обучения MyEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: ({text: ()}, ()), types: ({text: tf.string}, tf.int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_ds = snips_ds.repeat(1050) # если repeat не передать аргументов, то только MyEstimator сам сможет остановить обучение\n",
    "snips_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый элемент датасета тоже по сути маленький датасет, но есть и отличия в методах, которые как-то явно в паре слов не описать. На примере батчевания можно сказать, что hub.text_embedding_column ждёт только батчей, поэтому как минимум .batch(1) к датасету нужно применить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ({text: (?,)}, (?,)), types: ({text: tf.string}, tf.int64)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_ds = snips_ds.batch(4) # здесь нужно быть внимательным, потому что это влияет на размерность тензоров, и даже\n",
    "snips_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ещё несколько методов:\n",
    ".apply() (похож на .map(), но может делать преобразования всего датасета, а не поэлементно);\n",
    ".shuffle() (понятно, что делает);\n",
    "однако непонятно, какие есть ограничения на порядок вызова этих методов на датасете.\n",
    "\n",
    "Попробуем скормить наш датасет MyEstimator, на этот раз используем болванку - BaselineClassifier - который вообще не использует фичи, а только лейблы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpnz8nv0gg\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpnz8nv0gg', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdef87f1860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "my_estimator = tf.estimator.BaselineClassifier(n_classes=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_input_fn, которая передаётся в MyEstimator.train() должна возвращать просто объект класса tf.data.Dataset. Нужно именно писать отдельную функцию, а не передавать заранее сконструированный датасет через лямбда-функцию; связано это с процедурой построение графа для MyEstimator.\n",
    "\n",
    "input_fn не должна принимать аргументов, но если очень захотеть, то можно использовать ту же лямбду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpnz8nv0gg/model.ckpt.\n",
      "INFO:tensorflow:loss = 13.621372, step = 1\n",
      "INFO:tensorflow:Loss for final step: 13.621372.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.baseline.BaselineClassifier at 0x7fdef87f1908>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def snips_train_input_fn():\n",
    "    snips_ds = tf.data.TextLineDataset(train_datapath).skip(1)\n",
    "    snips_ds = snips_ds.map(_parse_line)\n",
    "    snips_ds = snips_ds.batch(32)\n",
    "    return snips_ds\n",
    "my_estimator.train(input_fn=snips_train_input_fn, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-12:54:22\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpnz8nv0gg/model.ckpt-1\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-12:54:22\n",
      "INFO:tensorflow:Saving dict for global step 1: accuracy = 0.2857143, average_loss = 1.8057231, global_step = 1, loss = 12.640061\n"
     ]
    }
   ],
   "source": [
    "eval_info = my_estimator.evaluate(input_fn=snips_train_input_fn) # снова скорим на обучающей же выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.2857143, 'average_loss': 1.8057231, 'loss': 12.640061, 'global_step': 1}\n"
     ]
    }
   ],
   "source": [
    "print(eval_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
